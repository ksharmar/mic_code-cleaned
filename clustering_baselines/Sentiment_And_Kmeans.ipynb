{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StanceEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      website    fact  object  sentiment_score\n",
      "0  1003856029  N_E312  N_E312          -0.6597\n",
      "1  1013430282  N_E312  N_E312          -0.7149\n",
      "2  1014445069  N_E312  N_E312          -0.7149\n",
      "3   101596871  N_E312  N_E312           0.5267\n",
      "4    10228272  N_E312  N_E312          -0.7412\n",
      "Index([u'website', u'fact', u'object', u'sentiment_score'], dtype='object')\n",
      "                       fact       website  sentiment_score\n",
      "0               N_Airfrance  2.313271e+07        -0.313637\n",
      "1                N_Airliner  1.396921e+07         0.129432\n",
      "2                  N_Amanda  2.438650e+07        -0.103014\n",
      "3                 N_AnnieLe  4.059135e+07        -0.194677\n",
      "4  N_BarnesNobleObamaMonkey  1.760870e+07         0.238274\n",
      "Index([u'fact', u'website', u'sentiment_score'], dtype='object')\n",
      "992\n",
      "992\n",
      "   accuracy  f1-score  precision    recall  support\n",
      "0  0.469758  0.532028   0.474603  0.605263      494\n",
      "1  0.469758  0.388372   0.461326  0.335341      498\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def _metrics_report_to_df(ytrue, ypred):\n",
    "    precision, recall, fscore, support = metrics.precision_recall_fscore_support(ytrue, ypred)\n",
    "    acc = metrics.accuracy_score(ytrue, ypred)\n",
    "    classification_report = pd.concat(map(pd.DataFrame, [[acc,acc], fscore, precision, recall, support]), axis=1)\n",
    "    classification_report.columns = [\"accuracy\", \"f1-score\", \"precision\", \"recall\", \"support\"]\n",
    "    return(classification_report)\n",
    "\n",
    "import pandas as pd\n",
    "# data=\"kwon/sentiments.txt\"\n",
    "data=\"twitter-ma/twitter-ma.csv\"\n",
    "\n",
    "df = pd.read_csv(data)\n",
    "print df.head()\n",
    "print df.columns\n",
    "# sys.exit()\n",
    "\n",
    "senti = df.groupby(['fact'], as_index=False).mean()\n",
    "label = (senti['sentiment_score']>0)*1\n",
    "\n",
    "print senti.head()\n",
    "print senti.columns\n",
    "print len(label)\n",
    "\n",
    "ground = []\n",
    "for row in senti['fact']:\n",
    "    if row.split(\"_\")[0] == 'N':\n",
    "        ground.append(0)\n",
    "    else:\n",
    "        ground.append(1)\n",
    "print len(ground)\n",
    "        \n",
    "report = _metrics_report_to_df(ground, label) # metrics.classification_report(labels, assigned, target_names=self.keys, output_dict=True)\n",
    "pprint(report)       \n",
    "\n",
    "# Kwon\n",
    "\n",
    "# accuracy  f1-score  precision    recall  support\n",
    "# 0  0.531532  0.527273   0.491525  0.568627       51\n",
    "# 1  0.531532  0.535714   0.576923  0.500000       60\n",
    "\n",
    "# MA\n",
    "# accuracy  f1-score  precision    recall  support\n",
    "# 0  0.486218  0.572477   0.474886  0.720554      433\n",
    "# 1  0.486218  0.356354   0.516000  0.272152      474\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user 233719 592391.0\n",
      "(992, 4)\n",
      "[597.1683467741935, 1982.9470718525986, 64.61494570103345, 0.16573996807935304]\n",
      "992\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.25      0.33       494\n",
      "         1.0       0.51      0.77      0.61       498\n",
      "\n",
      "    accuracy                           0.51       992\n",
      "   macro avg       0.51      0.51      0.47       992\n",
      "weighted avg       0.51      0.51      0.47       992\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.75      0.60       494\n",
      "         1.0       0.49      0.23      0.31       498\n",
      "\n",
      "    accuracy                           0.49       992\n",
      "   macro avg       0.49      0.49      0.45       992\n",
      "weighted avg       0.49      0.49      0.45       992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of posts in the cascade\n",
    "# Time: Total time length of cascade, Avg time gap between posts in cascade\n",
    "## ignore tree: Avg Depth of tree, Avg Breadth of tree \n",
    " \n",
    "# data=\"twitter-ma/train_cascades.txt\"\n",
    "# ground_data=\"twitter-ma/train_labels.txt\"\n",
    "# data=\"train_cascades.txt\"\n",
    "# ground_data=\"train_labels.txt\"\n",
    "\n",
    "# data=\"../data/kwon/cascades.txt\"\n",
    "# ground_data=\"../data/kwon/labels.txt\"\n",
    "\n",
    "data=\"../data/tma/cascades.txt\"\n",
    "ground_data=\"../data/tma/labels.txt\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import operator, sys\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def extract_feats(ulist, tlist, top_users_set):\n",
    "    total_time = tlist[-1]\n",
    "    num_posts = len(ulist)\n",
    "    time_gap = tlist[1:] - tlist[0:-1]\n",
    "    frac_top_users = len(set(ulist) & top_users_set)*1.0/len(ulist)\n",
    "    return [num_posts, total_time, np.mean(time_gap), frac_top_users]\n",
    "    \n",
    "    # feats = extract_feats(np.array([1,2,3]), np.array([2,5,6]))\n",
    "\n",
    "\n",
    "cascades = []\n",
    "upart = {}\n",
    "\n",
    "tot_eng = 0\n",
    "for line in open(data):\n",
    "    # print(line)\n",
    "    activations = line.strip().split(\",\")\n",
    "    tot_eng += len(activations)/2\n",
    "    ulist, tlist = [], []\n",
    "    \n",
    "    users = activations[0::2]\n",
    "    times = activations[1::2]\n",
    "    \n",
    "    for u, t in zip(users, times):\n",
    "        u = int(u)\n",
    "        t = float(t)\n",
    "        if u not in upart:\n",
    "            upart[u] = 1\n",
    "        else: upart[u] += 1\n",
    "        ulist.append(u); tlist.append(float(t))    \n",
    "    cascades.append((ulist, tlist))\n",
    "    # break\n",
    "print(\"user\", len(upart), tot_eng)\n",
    "# sys.exit()\n",
    "    \n",
    "sorted_x = sorted(upart.items(), key=operator.itemgetter(1), reverse=True)\n",
    "top_users = sorted_x[:5000]\n",
    "top_users_set = set()\n",
    "for u, count in top_users:\n",
    "    top_users_set.add(u)\n",
    "\n",
    "X = []\n",
    "for ulist, tlist in cascades:\n",
    "    cas_features = extract_feats(np.array(ulist), np.array(tlist), top_users_set)\n",
    "    X.append(cas_features)\n",
    "\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "score = np.mean(X, axis=0)\n",
    "score[1] = score[1]/3600.0; score[2] = score[2]/3600.0 \n",
    "print(list(score)) # [num_posts, total_time, np.mean(time_gap), frac_top_users]\n",
    "# sys.exit()\n",
    "# print X\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=None).fit(preprocessing.normalize(X))\n",
    "# kmeans = KMeans(n_clusters=2, random_state=None).fit(X)\n",
    "assigned = kmeans.labels_# array([0, 0, 0, 1, 1, 1], dtype=int32)\n",
    "# print assigned\n",
    "print(len(assigned))\n",
    "\n",
    "ground = np.loadtxt(ground_data)\n",
    "report = metrics.classification_report(ground, assigned) # metrics.classification_report(labels, assigned, target_names=self.keys, output_dict=True)\n",
    "print(report)\n",
    "report = metrics.classification_report(ground, 1-assigned) # metrics.classification_report(labels, assigned, target_names=self.keys, output_dict=True)\n",
    "print(report)\n",
    "# print kmeans.predict([[0, 0], [4, 4]]) # array([0, 1], dtype=int32)\n",
    "# print kmeans.cluster_centers_ # array([[1., 2.], [4., 2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.75      0.60       494\n",
      "         1.0       0.49      0.23      0.31       498\n",
      "\n",
      "    accuracy                           0.49       992\n",
      "   macro avg       0.49      0.49      0.45       992\n",
      "weighted avg       0.49      0.49      0.45       992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = metrics.classification_report(ground, 1 - assigned)\n",
    "print(report)\n",
    "np.savetxt('../output/baselines/tma/kmeans_pred_results.txt', 1 - assigned, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF predictions fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_Vince\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "   \n",
    "\n",
    "datadir = 'kwon'\n",
    "labels = '../data/kwon/labels.txt'\n",
    "cascade_names = '../data/kwon/cascade_names.txt'\n",
    "save_file = '../output/baselines/kwon/tf_pred_results.txt'\n",
    "\n",
    "# datadir = 'tma'\n",
    "# labels = '../data/tma/labels.txt'\n",
    "# cascade_names = '../data/tma/cascade_names.txt'\n",
    "# save_file = '../output/baselines/tma/tf_pred_results.txt'\n",
    "\n",
    "dict_preds = {}\n",
    "\n",
    "if datadir == 'kwon':\n",
    "    r = pkl.load(open('../clustering_baselines/tf_kwon_results.pkl', 'rb'))\n",
    "    scores = r['dataframe']['trustworthiness']\n",
    "    events = r['dataframe']['fact']\n",
    "    for event, score in zip(events, scores):\n",
    "        # event, score = line.split(',')[0].strip(), float(line.split(',')[1].strip().strip('[]'))\n",
    "        dict_preds[event] = (score < 0)*1\n",
    "        # break\n",
    "elif datadir == 'tma':\n",
    "    f = open('../output/baselines/tma/tf_predictions.txt', 'r')\n",
    "    for line in f.readlines():\n",
    "        if line == '' or line == '\\n':\n",
    "            continue\n",
    "        # print(line)\n",
    "        event, score = line.split(',')[0].strip(), float(line.split(',')[1].strip().strip('[]'))\n",
    "        # print(event, score)\n",
    "        dict_preds[event] = (score < 0)*1\n",
    "        # break\n",
    "    # print(scores)\n",
    "print(list(dict_preds.keys())[0])\n",
    "dict_preds['N_Vince']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.82      0.61        51\n",
      "         1.0       0.62      0.25      0.36        60\n",
      "\n",
      "    accuracy                           0.51       111\n",
      "   macro avg       0.55      0.54      0.48       111\n",
      "weighted avg       0.56      0.51      0.47       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "cnames = np.loadtxt(cascade_names, dtype=np.str)\n",
    "gt = np.loadtxt(labels)\n",
    "# print(cnames)\n",
    "# # print(gt)\n",
    "\n",
    "tar = []\n",
    "preds = []\n",
    "count0 = 0\n",
    "for i, name in enumerate(cnames):\n",
    "    n = re.sub('.txt$', '', name)\n",
    "    if n in dict_preds:\n",
    "        p = dict_preds[n]\n",
    "        preds.append(p)\n",
    "    else:\n",
    "        preds.append(0)\n",
    "        count0 += 1\n",
    "\n",
    "print(count0)\n",
    "print(metrics.classification_report(gt, preds))\n",
    "\n",
    "np.savetxt(save_file, preds, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
