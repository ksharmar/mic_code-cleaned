{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import numpy as np\n",
    "from notebook_utils import *\n",
    "from parameter_estimation import load_data_util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import snap\n",
    "import operator\n",
    "from sklearn import metrics\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rc('font', family='serif', size='13') # controls default text sizes\n",
    "plt.rc('xtick', labelsize='12')\n",
    "plt.rc('ytick', labelsize='12')\n",
    "plt.rc('legend', fontsize='13') # legend fontsize\n",
    "plt.rc('axes', labelsize='12', titlesize='13')  # fontsize of the x and y labels # fontsize of the axes title\n",
    "# plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract tweets features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done till 0 67\n",
      "done till 90 64\n",
      "done till 180 17\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\"\"\"A script for downloading (crawling) tweets by their IDs.\"\"\"\n",
    "\n",
    "import tweepy\n",
    "import pickle as pkl\n",
    "\n",
    "consumerkey = 'zg5ZoWps3wHjfAwjmR5mQHjKW'\n",
    "consumersecret = 'dsCwQ2Da7ajUTj4IWHTEUY0owJ0GxZOiGVM9Lr0529RqEiRk31'\n",
    "accesstoken = '936005824111026176-WfU8grlXW2ogKis6vmB1rC6dcAquaCO'\n",
    "accesssecret = 'nJZBE8h2Y8aloEZnJ4Fuwq3MFJvf6E4BVRx41lwbXqpyc'\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    auth = tweepy.OAuthHandler(consumerkey, consumersecret)\n",
    "    auth.set_access_token(accesstoken, accesssecret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    # in_path = './untitled.txt'\n",
    "    # out_path = './testoutput.pkl'\n",
    "    \n",
    "    in_path = '../output/all_kwon/kwon/infl_uids_tweetids.txt'\n",
    "    out_path = '../output/all_kwon/kwon/output_status_list_infl_uids_tweetids.pkl'\n",
    "    step = 90\n",
    "    \n",
    "    all_tweets = []\n",
    "    # tweet_ids = []\n",
    "    tweet_ids = list(np.loadtxt(in_path, dtype=np.int64)[:, 1])\n",
    "#     f = open(in_path, 'r') \n",
    "#     for line in f.readlines():\n",
    "#         id = int(line.strip().split()[0])\n",
    "#         tweet_ids.append(id)\n",
    "        \n",
    "    for i in range(0, len(tweet_ids), step):\n",
    "        tweets = api.statuses_lookup(tweet_ids[i:i+step])\n",
    "        all_tweets += tweets\n",
    "        # break\n",
    "        print('done till', i, len(tweets))\n",
    "        \n",
    "    pkl.dump(all_tweets, open(out_path, 'wb'))\n",
    "        \n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 117 ('Scooopy', 'http://t.co/4ReKy9iiDb - Scoops about Technology & Gadgets', False)\n"
     ]
    }
   ],
   "source": [
    "tweet_ids = list(np.loadtxt('../output/all_kwon/kwon/infl_uids_tweetids.txt', dtype=np.int64)[:, 1])\n",
    "uids = list(np.loadtxt('../output/all_kwon/kwon/infl_uids_tweetids.txt', dtype=np.int64)[:, 0])\n",
    "mapp_twid_uid = dict(zip(tweet_ids, uids)) \n",
    "\n",
    "all_tweets = pkl.load(open('../output/all_kwon/kwon/output_status_list_infl_uids_tweetids.pkl', 'rb'))\n",
    "dict_ufeats = {}\n",
    "for status in all_tweets:\n",
    "    s = status._json\n",
    "    twid = s['id']\n",
    "    uid = mapp_twid_uid[twid]\n",
    "    # pprint(s)\n",
    "    uobj = s['user']\n",
    "    # uid = uobj['id']  # not anonymized, I have anonymized ones in KWON dataset\n",
    "    desc = uobj['description']\n",
    "    name = uobj['screen_name']\n",
    "    veri = uobj['verified']\n",
    "    tup = (name, desc, veri)\n",
    "    dict_ufeats[uid] = tup\n",
    "    # print(uid)\n",
    "    # break\n",
    "    # dict_texts[s['id']] = s['text']\n",
    "print(len(all_tweets), len(dict_ufeats), next(iter(dict_ufeats.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39357, 41645179, 51828921, 1688, 41625921] [3691337196, 1844105043, 3599725603, 3869391368, 1752867728]\n"
     ]
    }
   ],
   "source": [
    "# extract tweet ids from anonymized user ids\n",
    "\n",
    "cascades_file = '../data/kwon/cascades.txt'\n",
    "cascade_names = '../data/kwon/cascade_names.txt'\n",
    "fil_dir = 'kwon/Tweets/'\n",
    "\n",
    "train_cascades = read_cascades_file(cascades_file)\n",
    "casnames = np.loadtxt(cascade_names, dtype=np.str)\n",
    "\n",
    "def mapping_uidx_to_cid(train_cascades):\n",
    "    dict_cids = {}\n",
    "    for cid, cas in enumerate(train_cascades):\n",
    "        users = cas[:, 0]\n",
    "        for u in users: dict_cids[int(u)]=cid\n",
    "    return dict_cids\n",
    "\n",
    "def get_tweetid_for_uid_from_eventfile(fil_dir, name, uid):\n",
    "    file_path = fil_dir + name\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None, usecols=range(4))\n",
    "    subdf = df[df[0].isin([uid])]\n",
    "    tweet_id = subdf.iloc[0, 1]\n",
    "    return tweet_id\n",
    "\n",
    "def get_list_infl_users():\n",
    "    # kwon\n",
    "    fake_infl = [39357, 41645179, 51828921, 1688, 41625921, 904660, 55640892, 45357704, 117511, 31745148, 49130864, 46283660, 31441663, 46161420, 54595164, 39025741, 16983873, 10045, 51489852, 53382638, 19771216, 18987, 85853, 39003758, 26101679, 54795103, 31749799, 40531228, 11402038, 31665361, 2187715, 54888609, 55762225, 12919420, 26206026, 15191995, 44083204, 9866568, 892, 33019416, 4270645, 36020039, 47509244, 49614, 51093837, 45924090, 599, 9581, 34949244, 30715157, 52174399, 34664714, 40179, 45241472, 600250, 26869787, 33498358, 88703, 45887855, 46849, 34753016, 12186498, 56001060, 81502, 10179069, 10054, 25980076, 37733084, 28768064, 51631070, 29708909, 23813323, 46643782, 56009279, 46060230, 2447156, 40134854, 55922657, 17236288, 39758253, 73058, 14808579, 36503735, 43569984, 2267067, 32691858, 41792135, 34882960, 25900052, 47386549, 46730408, 12944483, 9456, 34949531, 10278418, 29030843, 45708734, 15035771, 47419769, 1761233]\n",
    "    true_infl = [41645179, 51828921, 53471447, 46161420, 54888609, 31745148, 31441663, 117511, 904660, 18987, 15446395, 44282988, 53382638, 51489852, 48439353, 1709, 85853, 26206026, 25532591, 54425344, 45062504, 17512677, 39003758, 31665361, 31749799, 1217410, 2447156, 19771216, 31572449, 15191995, 2187715, 53272550, 48455831, 20155, 16757978, 12919420, 47110694, 44083204, 28555, 45708734, 30889300, 41792135, 1041957, 45887855, 49614, 33603546, 55922657, 13080305, 14808579, 48151623, 892, 73058, 4270645, 61994, 33986646, 46060230, 46849, 47419769, 34891224, 45241472, 48428454, 10278418, 25980076, 48872364, 35478, 31318992, 43981298, 51093837, 55848798, 34711253, 13420494, 9456, 443113, 37484955, 46730408, 5770411, 46931589, 8990, 26101679, 40125, 37733084, 41736970, 53703886, 46295098, 32691858, 37471, 7836260, 27747934, 55778501, 16983873, 26385432, 40864273, 40179, 34882960, 48350639, 46493916, 47386549, 27588149, 31480592, 40074055]\n",
    "    return fake_infl + true_infl\n",
    "\n",
    "def find_tweetid_for_userid(list_uids):\n",
    "    list_tweetids = []\n",
    "    for uid in list_uids:\n",
    "        cid = dict_cids[uid]\n",
    "        name = casnames[int(cid)]\n",
    "        # find tweet id from cascade file corresponding to this userid.\n",
    "        tweet_id = get_tweetid_for_uid_from_eventfile(fil_dir, name, uid)\n",
    "        list_tweetids.append(tweet_id)\n",
    "        # break\n",
    "    return list_tweetids\n",
    "\n",
    "dict_cids = mapping_uidx_to_cid(train_cascades)\n",
    "list_uids = get_list_infl_users()\n",
    "list_tweetids = find_tweetid_for_userid(list_uids)\n",
    "print(list_uids[0:5], list_tweetids[0:5])\n",
    "ext = np.vstack([list_uids, list_tweetids]).transpose()\n",
    "np.savetxt('../output/all_kwon/kwon/infl_uids_tweetids.txt', ext, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Features Extraction (Influential Users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num cascades 111 111\n",
      "t/f 51 60\n",
      "u_t, u_f, tot 76041 47869 123910\n",
      "users large eng > 5 2457\n",
      "users large eng > 10 871\n",
      "considered users in inference 2930\n",
      "num selected infl 100\n",
      "[   39357 41645179 51828921     1688 41625921   904660 55640892 45357704\n",
      "   117511 31745148 49130864 46283660 31441663 46161420 54595164 39025741\n",
      " 16983873    10045 51489852 53382638 19771216    18987    85853 39003758\n",
      " 26101679 54795103 31749799 40531228 11402038 31665361  2187715 54888609\n",
      " 55762225 12919420 26206026 15191995 44083204  9866568      892 33019416\n",
      "  4270645 36020039 47509244    49614 51093837 45924090      599     9581\n",
      " 34949244 30715157 52174399 34664714    40179 45241472   600250 26869787\n",
      " 33498358    88703 45887855    46849 34753016 12186498 56001060    81502\n",
      " 10179069    10054 25980076 37733084 28768064 51631070 29708909 23813323\n",
      " 46643782 56009279 46060230  2447156 40134854 55922657 17236288 39758253\n",
      "    73058 14808579 36503735 43569984  2267067 32691858 41792135 34882960\n",
      " 25900052 47386549 46730408 12944483     9456 34949531 10278418 29030843\n",
      " 45708734 15035771 47419769  1761233]\n",
      "[41645179 51828921 53471447 46161420 54888609 31745148 31441663   117511\n",
      "   904660    18987 15446395 44282988 53382638 51489852 48439353     1709\n",
      "    85853 26206026 25532591 54425344 45062504 17512677 39003758 31665361\n",
      " 31749799  1217410  2447156 19771216 31572449 15191995  2187715 53272550\n",
      " 48455831    20155 16757978 12919420 47110694 44083204    28555 45708734\n",
      " 30889300 41792135  1041957 45887855    49614 33603546 55922657 13080305\n",
      " 14808579 48151623      892    73058  4270645    61994 33986646 46060230\n",
      "    46849 47419769 34891224 45241472 48428454 10278418 25980076 48872364\n",
      "    35478 31318992 43981298 51093837 55848798 34711253 13420494     9456\n",
      "   443113 37484955 46730408  5770411 46931589     8990 26101679    40125\n",
      " 37733084 41736970 53703886 46295098 32691858    37471  7836260 27747934\n",
      " 55778501 16983873 26385432 40864273    40179 34882960 48350639 46493916\n",
      " 47386549 27588149 31480592 40074055]\n",
      "-------> fake infl feats\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KWON\n",
    "# --------------\n",
    "fake_component = 0\n",
    "data = \"kwon\"\n",
    "data_dir = \"../data/{}/\".format(data)\n",
    "cascades_file = data_dir + 'cascades.txt'\n",
    "labels_file = data_dir + 'labels.txt'\n",
    "cascade_names = data_dir + 'cascade_names.txt'\n",
    "\n",
    "userfeat_file = 'kwon_dataset/sub_user_info_share.txt'\n",
    "\n",
    "output = \"all_kwon/kwon\"\n",
    "output_dir = \"../output/{}/\".format(output)\n",
    "idx2u_file = output_dir + 'idx2u.txt'\n",
    "selected_infl_file = output_dir + 'selected_influential_users.tsv'\n",
    "casnames = np.loadtxt(cascade_names, dtype=np.str)\n",
    "\n",
    "train_cascades = read_cascades_file(cascades_file)\n",
    "train_labels = np.loadtxt(labels_file)\n",
    "print('num cascades', len(train_cascades), len(train_labels))\n",
    "u_t, u_f, sorted_users = eng_count(train_cascades, train_labels)\n",
    "fake_ind = np.where(train_labels == 1)[0]\n",
    "fake_cascades = np.array(train_cascades)[fake_ind]\n",
    "\n",
    "ufeat = pd.read_csv(userfeat_file, sep='\\t', header=None) # followers, followees (friends), posts\n",
    "ufeat.columns = ['userid', 'followers', 'friends/followees', 'posts']\n",
    "\n",
    "idx2u = np.loadtxt(idx2u_file)\n",
    "print('considered users in inference', len(idx2u))\n",
    "\n",
    "inf_df = pd.read_csv(selected_infl_file, sep='\\t', header=0)\n",
    "print('num selected infl', len(inf_df))\n",
    "fake_infl = inf_df['selected_{}'.format(fake_component)].apply(lambda x: int(idx2u[x]))\n",
    "true_infl = inf_df['selected_{}'.format(1-fake_component)].apply(lambda x: int(idx2u[x]))\n",
    "\n",
    "print(np.array(fake_infl))\n",
    "print(np.array(true_infl))\n",
    "\n",
    "# old infl users identified\n",
    "# fake_infl = [42921853, 45696250, 9428477, 18485804, 37484955, 15538468, 34544545, 80876, 54879, 33361052, 37232745, 33603546, 1680247, 40864273, 134, 20164964, 29144327, 1883240, 47924370, 24916432, 35560818, 15554302, 773, 5272341, 29507615, 29156372, 38769438, 34345266, 2233, 8627533, 14611855, 32269208, 49726906, 28193335, 8098832, 12200968, 55903340, 47588713, 31917878, 19707732, 46248155, 33910097, 55976845, 27047346, 47386549, 9910421, 22678269, 44147043, 35379119, 51841996, 20795579, 37955867, 1002, 46900340, 48467139, 55970765, 17866088, 20853985, 21684, 55972805, 17281580, 18797761, 47872293, 22601310, 7148912, 43816406, 103535, 30331073, 27958549, 17119043, 34686602, 30721844, 28131892, 12068845, 18035830, 27104465, 22978489, 47977103, 46730321, 43369375, 43756699, 12186498, 19444022, 61280, 41050967, 31441663, 42926119, 55991030, 10688944, 16358632, 23219078, 5297007, 2150040, 35332369, 5773920, 68059, 42401663, 27547127, 15491837, 4667953]\n",
    "# true_infl = [30122132, 9456, 1041957, 55931092, 52656642, 51298452, 2329550, 74679, 9761160, 42598690, 12987474, 1083172, 25480395, 24007123, 53616317, 6934529, 22041886, 25869801, 20510188, 37057701, 42946550, 23803578, 21417341, 31665361, 29264958, 16639123, 28117383, 5396879, 30971958, 23086420, 28776904, 80876, 40591, 49782598, 48911973, 10501473, 9022728, 11494548, 47350415, 55774980, 39515674, 42837, 17072, 49272331, 81419, 42760, 24058200, 10041, 30404702, 6365957, 11749083, 25672074, 14748530, 52263483, 166747, 45543326, 15315324, 20716299, 5903540, 34420488, 40075183, 23246755, 56017762, 43560260, 15191995, 33786128, 360451, 25200040, 16939, 23539100, 16759010, 56023202, 3420957, 9645533, 55935602, 41127254, 7414712, 54917, 52244910, 42976, 29156372, 24773084, 32106161, 35444685, 40176, 46945086, 54780470, 80877, 49755942, 19047950, 9656071, 38410, 11209421, 6481132, 49614, 35775165, 21719470, 22525981, 65561, 825]\n",
    "# print('fake infl list', list(fake_infl))\n",
    "# print('true infl list', list(true_infl))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Second: Find features of influential users selected. # followers, # posts, description # eng_t # eng_f\n",
    "\"\"\"\n",
    "\n",
    "def get_cas_names(uid):\n",
    "    names_list = []\n",
    "    for tr_cas, name in zip(fake_cascades, casnames[fake_ind]):\n",
    "        users = set(tr_cas[:, 0])\n",
    "        if uid in users:\n",
    "            names_list.append(name)\n",
    "        # break\n",
    "    return names_list\n",
    "        \n",
    "def extract_feats(infl, u_t, u_f, strname):\n",
    "    extracted_df = ufeat[ufeat['userid'].isin(infl)]\n",
    "    extracted_df['u_t'] = extracted_df['userid'].apply(lambda x: u_t[x] if x in u_t else 0)\n",
    "    extracted_df['u_f'] = extracted_df['userid'].apply(lambda x: u_f[x] if x in u_f else 0)\n",
    "    # print(infl, infl[0], type(infl[0]))\n",
    "    extracted_df['features'] = [dict_ufeats[int(uid)] if int(uid) in dict_ufeats else () for uid in extracted_df['userid']]\n",
    "    \n",
    "    extracted_df['casnames'] = [get_cas_names(uid) for uid in extracted_df['userid']]\n",
    "    extracted_df = extracted_df.set_index('userid').loc[infl]\n",
    "    # print(extracted_df.head(5))\n",
    "    # print(extracted_df.head(30))\n",
    "    # extracted_df.to_csv('../output/all_kwon/kwon/extracted_user_feats_{}.csv'.format(strname))\n",
    "    \n",
    "\n",
    "# print('-------> true infl feats\\n')\n",
    "# extract_feats(true_infl, u_t, u_f, 'true_infl')\n",
    "print('-------> fake infl feats\\n')\n",
    "extract_feats(fake_infl, u_t, u_f, 'fake_infl')\n",
    "# print('-------> random users \\n')\n",
    "# np.random.seed(145)\n",
    "# random_users = [idx2u[c] for c in np.random.choice(len(idx2u), len(true_infl), replace=False)]\n",
    "# extract_feats(random_users, u_t, u_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hendricius', 'Software engineer + geek', False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_ufeats[2267067]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_f[2267067]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
